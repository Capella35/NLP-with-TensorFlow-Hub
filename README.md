# NLP-with-TensorFlow-Hub

Why should transfer learning work in NLP?

Many NLP tasks share common knowledge about language (linguistic representations, structural similarities, syntax, semantics...).

Annotated data is rare, make use of as much supervision as possible. If you can combine data sets that you used for several tasks to get much bigger datasets. Bigger datasets are generally better for deep learning models.

Unlabelled data is abundant (e.g. on the world wide web) and one should try to use as much of it as possible. 

Empirically, transfer learning has resulted in SOTA results for many supervised NLP tasks (e.g. classification, information extraction, Q&A, etc).
